{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "embsize = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Corpus_path = '../data/Corpus.txt'\n",
    "Model_path = 'model/w2vModel3.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 16:15:37,532 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2021-03-26 16:15:37,534 : INFO : collecting all words and their counts\n",
      "2021-03-26 16:15:37,535 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-03-26 16:15:37,584 : INFO : PROGRESS: at sentence #10000, processed 51683 words, keeping 8029 word types\n",
      "2021-03-26 16:15:37,617 : INFO : collected 12094 word types from a corpus of 88871 raw words and 17222 sentences\n",
      "2021-03-26 16:15:37,625 : INFO : Loading a fresh vocabulary\n",
      "2021-03-26 16:15:37,647 : INFO : effective_min_count=1 retains 12094 unique words (100% of original 12094, drops 0)\n",
      "2021-03-26 16:15:37,654 : INFO : effective_min_count=1 leaves 88871 word corpus (100% of original 88871, drops 0)\n",
      "2021-03-26 16:15:37,710 : INFO : deleting the raw counts dictionary of 12094 items\n",
      "2021-03-26 16:15:37,711 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2021-03-26 16:15:37,712 : INFO : downsampling leaves estimated 56027 word corpus (63.0% of prior 88871)\n",
      "2021-03-26 16:15:37,750 : INFO : estimated required memory for 12094 words and 5 dimensions: 6530760 bytes\n",
      "2021-03-26 16:15:37,751 : INFO : resetting layer weights\n",
      "2021-03-26 16:15:41,843 : INFO : training model with 15 workers on 12094 vocabulary and 5 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-03-26 16:15:41,934 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-03-26 16:15:41,940 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-03-26 16:15:41,940 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-03-26 16:15:41,943 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-03-26 16:15:41,952 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-03-26 16:15:41,954 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-03-26 16:15:41,974 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-03-26 16:15:41,979 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-03-26 16:15:41,980 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-03-26 16:15:41,988 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-03-26 16:15:41,995 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-03-26 16:15:42,005 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-26 16:15:42,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-26 16:15:42,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-26 16:15:42,018 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-26 16:15:42,018 : INFO : EPOCH - 1 : training on 88871 raw words (56014 effective words) took 0.2s, 341096 effective words/s\n",
      "2021-03-26 16:15:42,232 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-03-26 16:15:42,259 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-03-26 16:15:42,266 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-03-26 16:15:42,268 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-03-26 16:15:42,270 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-03-26 16:15:42,270 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-03-26 16:15:42,275 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-03-26 16:15:42,278 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-03-26 16:15:42,279 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-03-26 16:15:42,281 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-03-26 16:15:42,282 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-03-26 16:15:42,295 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-26 16:15:42,308 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-26 16:15:42,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-26 16:15:42,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-26 16:15:42,316 : INFO : EPOCH - 2 : training on 88871 raw words (56054 effective words) took 0.3s, 195431 effective words/s\n",
      "2021-03-26 16:15:42,644 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-03-26 16:15:42,656 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-03-26 16:15:42,660 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-03-26 16:15:42,664 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-03-26 16:15:42,669 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-03-26 16:15:42,671 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-03-26 16:15:42,673 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-03-26 16:15:42,676 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-03-26 16:15:42,680 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-03-26 16:15:42,683 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-03-26 16:15:42,685 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-03-26 16:15:42,688 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-26 16:15:42,690 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-26 16:15:42,713 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-26 16:15:42,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-26 16:15:42,722 : INFO : EPOCH - 3 : training on 88871 raw words (56110 effective words) took 0.4s, 143292 effective words/s\n",
      "2021-03-26 16:15:42,900 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-03-26 16:15:42,920 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-03-26 16:15:42,921 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-03-26 16:15:42,922 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-03-26 16:15:42,923 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-03-26 16:15:42,925 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-03-26 16:15:42,926 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-03-26 16:15:42,928 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-03-26 16:15:42,929 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-03-26 16:15:42,930 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-03-26 16:15:42,932 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-03-26 16:15:42,937 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-26 16:15:42,946 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-26 16:15:42,949 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-26 16:15:42,960 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-26 16:15:42,962 : INFO : EPOCH - 4 : training on 88871 raw words (55954 effective words) took 0.2s, 261828 effective words/s\n",
      "2021-03-26 16:15:43,240 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-03-26 16:15:43,243 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-03-26 16:15:43,244 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-03-26 16:15:43,245 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-03-26 16:15:43,247 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-03-26 16:15:43,248 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-03-26 16:15:43,249 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-03-26 16:15:43,251 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-03-26 16:15:43,251 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 16:15:43,252 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-03-26 16:15:43,253 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-03-26 16:15:43,255 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-26 16:15:43,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-26 16:15:43,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-26 16:15:43,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-26 16:15:43,266 : INFO : EPOCH - 5 : training on 88871 raw words (56056 effective words) took 0.3s, 191984 effective words/s\n",
      "2021-03-26 16:15:43,267 : INFO : training on a 444355 raw words (280188 effective words) took 1.4s, 197041 effective words/s\n",
      "2021-03-26 16:15:43,268 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2021-03-26 16:15:43,268 : INFO : storing 12094x5 projection weights into model/w2vModel3.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成\n"
     ]
    }
   ],
   "source": [
    "def A(train_path,model_path):\n",
    "    #首先打开需要训练的文本\n",
    "    shuju = open(train_path, 'r')\n",
    "    #通过Word2vec进行训练\n",
    "    model = Word2Vec(LineSentence(shuju), sg=1,size=embsize, window=5, min_count=1, workers=15,sample=1e-3)\n",
    "    #保存训练好的模型\n",
    "    model.wv.save_word2vec_format(model_path)\n",
    "    print('训练完成')\n",
    "    \n",
    "A(Train_Corpus_path,Model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 16:15:44,524 : INFO : loading projection weights from model/w2vModel3.txt\n",
      "2021-03-26 16:15:44,742 : INFO : loaded (12094, 5) matrix from model/w2vModel3.txt\n",
      "2021-03-26 16:15:44,745 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "[('亚运会', 0.9981467127799988), ('党工委', 0.9979308843612671), ('高级中学', 0.9972034692764282), ('织组', 0.9966740012168884), ('演员', 0.996593713760376), ('信阳', 0.9964818954467773), ('司务长', 0.9952391982078552), ('郴州', 0.9950400590896606), ('淮阴', 0.9949964284896851), ('张家口市', 0.9948666095733643)]\n",
      "********************\n",
      "法律\n",
      "********************\n",
      "0.944241\n",
      "********************\n",
      "[('会计学', 0.9995267987251282), ('新闻学', 0.9990501999855042), ('中南工业大学', 0.9983420372009277), ('大专', 0.9982876777648926), ('及', 0.9978668689727783), ('应用', 0.9978369474411011), ('中国', 0.9971964359283447), ('暨南大学', 0.9968695044517517), ('自动化', 0.9968068599700928), ('国际金融', 0.9966886043548584)]\n",
      "********************\n",
      "[-0.05453002  0.21871075  0.09479838 -0.09428279  0.07542817]\n"
     ]
    }
   ],
   "source": [
    "Mymodel = KeyedVectors.load_word2vec_format(Model_path, binary=False)\n",
    "\n",
    "print('********************')\n",
    "print(Mymodel.most_similar(positive=['香港', '经理'], negative=['学生'])  )\n",
    "#输出[('queen', 0.50882536), ...] \n",
    "print('********************')\n",
    "print(Mymodel.doesnt_match(\"罗湖区 南山区 法律 龙华区\".split())  )\n",
    "#输出'cereal' \n",
    "print('********************')\n",
    "print(Mymodel.similarity('局长','处长') )\n",
    "#输出0.73723527\n",
    "print('********************')\n",
    "print(Mymodel.most_similar('法律', topn=10) )\n",
    "print('********************')\n",
    "print(Mymodel['鄂南'] ) # raw numpy vector of a word  \n",
    "#输出array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "['双一流大学', '国企事业单位', '盐田', '科技技术', '卫生环保', '福田', '法务纪检', '市人大直属', '市政府直属', '宝安', '深圳1', '一般院校', '深圳', '市委直属', '龙岗', '龙华', '公安', '教育', '大鹏新区', '坪山', '市政协直属', '光明', '党务工作', '深圳市外1', '军检法机构', '理工方向1', '南山', '军检法机构1', '经济金融', '基层工作', '交通运输', '街道基层', '罗湖', '港澳外事']\n",
      "{'双一流大学': 0, '国企事业单位': 1, '盐田': 2, '科技技术': 3, '卫生环保': 4, '福田': 5, '法务纪检': 6, '市人大直属': 7, '市政府直属': 8, '宝安': 9, '深圳1': 10, '一般院校': 11, '深圳': 12, '市委直属': 13, '龙岗': 14, '龙华': 15, '公安': 16, '教育': 17, '大鹏新区': 18, '坪山': 19, '市政协直属': 20, '光明': 21, '党务工作': 22, '深圳市外1': 23, '军检法机构': 24, '理工方向1': 25, '南山': 26, '军检法机构1': 27, '经济金融': 28, '基层工作': 29, '交通运输': 30, '街道基层': 31, '罗湖': 32, '港澳外事': 33}\n"
     ]
    }
   ],
   "source": [
    "# 打开文件\n",
    "text_path = '../data/Corpus.txt'\n",
    "name_path = '../data/powerset.txt'\n",
    "with open(text_path,'r') as f:\n",
    "    texts = f.readlines()\n",
    "f.close()\n",
    "with open(name_path,'r') as g:\n",
    "    names = g.readlines()\n",
    "g.close()\n",
    "# 统计标签组\n",
    "label_set = []\n",
    "for name in names:\n",
    "    l = eval(name.split('\\t',2)[-1])\n",
    "    label_set.extend(l)\n",
    "label_set = list(set(label_set))\n",
    "print(len(label_set))\n",
    "label2id = {}\n",
    "for i,label in enumerate(label_set):\n",
    "    label2id[label] = i\n",
    "print(label_set)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建KG文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = open('../data/text2label.txt','w')\n",
    "for i in range(len(texts)):\n",
    "    _,flag,labels = names[i].split('\\t',2)\n",
    "    labels = eval(labels)\n",
    "    for label in labels:\n",
    "        h.write('{}\\ttext.2.label\\t{}\\n'.format(texts[i].strip(),label))\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = open('../data/limit.txt','w')\n",
    "l2l = [('双一流大学','基层工作'),('龙华','南山'),('港澳外事','龙华'),('军检法机构','双一流大学'),('卫生环保','公安')]\n",
    "for tu in l2l:\n",
    "    h.write('{}\\tlabel.reject.label\\t{}\\n'.format(tu[0],tu[1]))\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练图谱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tensor = {}\n",
    "Train_Corpus_path = '../data/Corpus.txt'\n",
    "Model_path = 'model/w2vModel.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 15:50:33,680 : INFO : loading projection weights from model/w2vModel.txt\n",
      "2021-03-26 15:50:36,886 : INFO : loaded (12094, 300) matrix from model/w2vModel.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading W2V Model: 3.207399845123291 s\n"
     ]
    }
   ],
   "source": [
    "# 导入训练练好的Word2Vec模型作初始化\n",
    "tk0 = time.time()\n",
    "Mymodel =  KeyedVectors.load_word2vec_format(Model_path, binary=False)\n",
    "print('Loading W2V Model:',time.time()-tk0,'s')\n",
    "#print(Mymodel['深圳'] ) # raw numpy vector of a word  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor = {}\n",
    "for key in label_set:\n",
    "    label_tensor[key] = np.random.rand(embsize)\n",
    "    label_tensor[key] = torch.Tensor(label_tensor[key])\n",
    "    label_tensor[key].requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_set = ['label.reject.label','text.2.label']\n",
    "relation_tensor = {}\n",
    "for key in relation_set:\n",
    "    relation_tensor[key] = np.random.rand(embsize)\n",
    "    relation_tensor[key] = torch.Tensor(relation_tensor[key])\n",
    "    relation_tensor[key].requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-1302086e2eaa>:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  word_tensor[item] = torch.Tensor(Mymodel[item])\n"
     ]
    }
   ],
   "source": [
    "word_tensor = {}\n",
    "for text in texts:\n",
    "    L = text.strip().split()\n",
    "    for item in L:\n",
    "        if item not in word_tensor:\n",
    "            word_tensor[item] = torch.Tensor(Mymodel[item])\n",
    "            word_tensor[item].requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 定义句子的embedding方法\n",
    "def doc_embd(string):\n",
    "    res = string.split()\n",
    "    result = 0\n",
    "    for item in res:\n",
    "        if item not in word_tensor:\n",
    "            word_tensor[item] = Mymodel[item]\n",
    "            word_tensor[item] = torch.Tensor(word_tensor[item])\n",
    "            word_tensor[item].requires_grad = True\n",
    "        result += word_tensor[item]\n",
    "    doc_embedding = result/len(res)\n",
    "    return doc_embedding\n",
    "\n",
    "zw = doc_embd('深圳')\n",
    "print(type(zw))\n",
    "#print(zw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['深圳 美都美容美发足浴公司 党支部 书记\\ttext.2.label\\t党务工作\\n', '深圳 美都美容美发足浴公司 党支部 书记\\ttext.2.label\\t深圳\\n', '驻 香港 部队 步兵旅 司令部 作训 科长\\ttext.2.label\\t港澳外事\\n', '驻 香港 部队 步兵旅 司令部 作训 科长\\ttext.2.label\\t军检法机构\\n', '驻 香港 部队 步兵旅 司令部 作训 科长\\ttext.2.label\\t深圳市外1\\n', '深圳市人民政府 办公厅 社会 一处 主任科员\\ttext.2.label\\t深圳\\n', '深圳市 节约用水 办公室 主任\\ttext.2.label\\t深圳\\n', '深圳市 综合交通设计研究院有限公司 设计支部委员会 副书记\\ttext.2.label\\t科技技术\\n', '深圳市 综合交通设计研究院有限公司 设计支部委员会 副书记\\ttext.2.label\\t交通运输\\n', '深圳市 综合交通设计研究院有限公司 设计支部委员会 副书记\\ttext.2.label\\t深圳\\n']\n"
     ]
    }
   ],
   "source": [
    "allfacts = []\n",
    "with open('../data/text2label.txt', 'r') as f:\n",
    "    allfacts.extend(f.readlines())\n",
    "f.close()\n",
    "with open('../data/limit.txt', 'r') as f:\n",
    "    allfacts.extend(f.readlines())\n",
    "f.close()\n",
    "print(allfacts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打分函数\n",
    "# TransE\n",
    "def TransE(he,re,te):\n",
    "    score =  torch.norm(he+re-te)\n",
    "    return score\n",
    "fscore = TransE\n",
    "# **********************************\n",
    "# Loss函数\n",
    "def Loss(L):\n",
    "    loss = 0\n",
    "    for fact in L:\n",
    "        h,r,t = fact.strip().split('\\t')\n",
    "        if h not in label_tensor:\n",
    "            he = doc_embd(h)\n",
    "        else:\n",
    "            he = label_tensor[h]\n",
    "        re = relation_tensor[r]\n",
    "        if t not in label_tensor:\n",
    "            te = doc_embd(h)\n",
    "        else:\n",
    "            te = label_tensor[t]\n",
    "        loss += fscore(he,re,te)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trainning(First Text): tensor([ 0.1442,  0.0740, -0.0135,  0.3450, -0.3631, -0.0803,  0.2139, -0.0635,\n",
      "        -0.0517,  0.1062, -0.0944, -0.3770, -0.0591, -0.0565, -0.2256,  0.1626,\n",
      "        -0.2106, -0.0445, -0.0650, -0.3243,  0.2887, -0.2291, -0.2039,  0.0546,\n",
      "         0.3436, -0.4006, -0.0976, -0.0695,  0.0367, -0.0282, -0.3366,  0.0137,\n",
      "         0.1549, -0.2584,  0.0992, -0.2385, -0.1299, -0.0764,  0.3816, -0.0926,\n",
      "        -0.3583,  0.1229,  0.3504,  0.1395, -0.4602,  0.1754,  0.0375,  0.0601,\n",
      "         0.1765,  0.0610, -0.5037, -0.3318, -0.0625, -0.1821,  0.1076, -0.2856,\n",
      "        -0.2178,  0.2934, -0.1109,  0.3356,  0.1511,  0.1882, -0.1756,  0.2165,\n",
      "        -0.0565,  0.0204,  0.1208, -0.0164, -0.0471, -0.3295, -0.1317,  0.3235,\n",
      "         0.3362,  0.1298, -0.3241, -0.3857, -0.4878, -0.0022,  0.2762, -0.3839,\n",
      "         0.2700,  0.2538,  0.0723,  0.1184, -0.6790, -0.1716,  0.1965,  0.1551,\n",
      "         0.3534,  0.2721,  0.3711,  0.0640, -0.1076, -0.0983,  0.0823,  0.1608,\n",
      "         0.0279,  0.1140, -0.0630, -0.2986,  0.1080, -0.1120,  0.1671, -0.3687,\n",
      "        -0.1262,  0.3338, -0.1528, -0.1927, -0.5948, -0.3250,  0.0668,  0.0114,\n",
      "         0.0668,  0.1334,  0.0822, -0.0597, -0.0553, -0.4493, -0.0182, -0.0371,\n",
      "         0.5600, -0.3332, -0.0298, -0.1346,  0.1053, -0.0453, -0.0771, -0.0224,\n",
      "         0.0119,  0.3489, -0.1565, -0.0633, -0.3823, -0.3412, -0.3873, -0.0161,\n",
      "        -0.3738,  0.1644, -0.0367, -0.0538, -0.0361, -0.0191,  0.0946, -0.4801,\n",
      "         0.0064,  0.3069,  0.5501,  0.1725,  0.2450, -0.0877, -0.2045,  0.1976,\n",
      "        -0.2987, -0.2008,  0.4526, -0.0551,  0.0598,  0.0626,  0.1555, -0.0192,\n",
      "        -0.5546, -0.2424, -0.0763,  0.0620, -0.0490, -0.3549, -0.2001, -0.4820,\n",
      "         0.0363,  0.3296, -0.5713, -0.6663,  0.0078,  0.0496,  0.3658,  0.1515,\n",
      "         0.2345,  0.5761, -0.0286, -0.1871,  0.3007, -0.4447, -0.2801, -0.3996,\n",
      "        -0.2892, -0.1647,  0.0785,  0.3892, -0.0599,  0.4816,  0.1552, -0.3959,\n",
      "         0.0244, -0.2165,  0.2687,  0.3370,  0.0595,  0.0284, -0.0367,  0.3794,\n",
      "         0.1143,  0.0386, -0.1063, -0.2095, -0.2672,  0.0809, -0.1594,  0.1636,\n",
      "         0.1182, -0.1192,  0.3388,  0.1073,  0.6108, -0.2655,  0.5606, -0.0487,\n",
      "         0.0272,  0.1959, -0.2096, -0.3366, -0.2944, -0.3095,  0.3625, -0.0370,\n",
      "        -0.1050, -0.2596, -0.1740, -0.4378, -0.1782,  0.4882, -0.2289,  0.4920,\n",
      "         0.4828,  0.0177, -0.3346,  0.2316, -0.0238, -0.1506,  0.0977,  0.1338,\n",
      "         0.1603, -0.0745, -0.2523,  0.2655,  0.1035,  0.1896,  0.5633, -0.0533,\n",
      "         0.0782,  0.0156,  0.1245,  0.1491,  0.3079,  0.0356, -0.0008,  0.2087,\n",
      "        -0.0713, -0.1157,  0.1007,  0.0242, -0.0645,  0.1935, -0.2504,  0.0015,\n",
      "         0.1355,  0.3436,  0.1968,  0.0223, -0.1047,  0.0968, -0.0412,  0.2478,\n",
      "        -0.0033,  0.1961, -0.2785,  0.3170,  0.2265, -0.2331, -0.0932,  0.0266,\n",
      "        -0.0768, -0.0125,  0.2096,  0.0527,  0.3801, -0.2560, -0.1783,  0.3845,\n",
      "         0.0991, -0.4440,  0.2395, -0.2888,  0.0590,  0.1400,  0.0397, -0.1203,\n",
      "        -0.0268,  0.2559, -0.0586,  0.1424], requires_grad=True)\n",
      "Epoch 0 - Batch Loss: tensor(1929.6812, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(1697.3853, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(1498.4352, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(1319.0352, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(1192.3002, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(1126.9604, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(1095.8281, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(1027.8553, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(984.5281, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(979.1328, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(975.3698, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(897.0388, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(858.0624, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(875.7188, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(824.0636, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(793.6868, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(819.0369, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(696.0830, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(792.1755, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(747.2565, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(751.6419, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(754.4387, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(683.7068, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(674.9360, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(673.1577, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(641.6097, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(653.6895, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(681.3419, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(656.5414, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(664.0094, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(605.1359, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(613.9237, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(654.7834, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(623.5031, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(643.4877, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(540.7985, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(629.7208, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(601.6339, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(598.8783, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(596.3167, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(633.7686, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(615.1347, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(625.6712, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(591.3508, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(630.9427, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(587.6397, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(558.4910, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(610.7781, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(574.7683, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(585.3676, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(558.9913, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(578.2942, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(643.1450, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(614.6155, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(587.4026, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(563.0113, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(592.3474, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(574.0175, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(569.3085, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(538.8334, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(584.2194, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(593.7344, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(596.5365, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(534.2308, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(519.9702, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(538.6104, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(583.6968, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(534.2766, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(513.5493, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(541.6473, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(513.1788, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(443.9324, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(569.7492, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(489.9053, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(567.1275, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(542.4352, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(429.6736, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(520.5277, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(549.5732, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(471.8956, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(482.1349, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(497.5406, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(498.2964, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Batch Loss: tensor(582.0579, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(447.8207, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(517.5084, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(541.9288, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(493.6698, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(483.7574, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(561.0714, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(495.1593, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(479.1983, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(479.0616, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(465.5667, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(469.0999, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(490.9201, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(498.7863, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(479.2881, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(466.1159, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(482.5034, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(416.3987, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(473.0702, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(466.5997, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(503.4617, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(448.1962, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(478.8244, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(452.3390, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(460.6506, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(453.8373, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(472.3926, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(423.5129, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(473.7265, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(479.1852, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(468.0145, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(450.2636, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(480.7502, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(476.7395, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(500.1888, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(376.5298, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(426.1634, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(429.2204, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(412.5573, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(507.3028, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(448.8609, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(416.4192, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(453.0567, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(451.6378, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(410.6054, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(404.4262, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(414.6918, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(481.8585, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(418.8465, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(439.0654, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(432.3074, grad_fn=<AddBackward0>)\n",
      "Epoch 0 - Batch Loss: tensor(95.7407, grad_fn=<AddBackward0>)\n",
      "Epoch 0 finished. Loss: 81854.16577911377\n",
      "Epoch 1 - Batch Loss: tensor(374.5621, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(428.8708, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(436.3099, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(438.3539, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(393.9245, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(435.2471, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(401.3007, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(451.2559, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(430.2314, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(422.9389, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(443.2068, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(419.8956, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(413.4958, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(434.4598, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(383.1324, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(353.8882, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(428.5485, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(384.6163, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(436.0577, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(415.7256, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(489.9554, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(395.1012, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(339.7639, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(456.1281, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(424.1197, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(398.7263, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(412.2404, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(402.8315, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(429.2814, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(395.3984, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(441.7120, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(463.3947, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(408.2170, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(400.5765, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(488.2645, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(407.1939, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(463.8669, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(437.9493, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(367.8195, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(435.3372, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(412.9341, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(389.3964, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(396.5460, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(420.5753, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(440.6219, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(404.2789, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(423.2587, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(406.8371, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(388.5977, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(400.3959, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(373.0786, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(391.1769, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(354.0650, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(398.9587, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(412.2516, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(357.9435, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(353.4640, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(417.1222, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(381.9565, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(369.3538, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(385.4589, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(396.5983, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(444.0493, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(320.9909, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(426.3095, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(335.1029, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(377.2684, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(377.0458, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(377.6060, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(363.2654, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(390.6295, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(366.2270, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(388.4105, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(381.9024, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(368.5550, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(337.8617, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(418.9861, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(347.0069, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Batch Loss: tensor(380.1288, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(353.2165, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(409.4811, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(425.4282, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(331.5325, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(408.5129, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(345.2845, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(398.1650, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(413.8993, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(405.2167, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(358.1163, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(415.5640, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(379.1413, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(358.9088, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(371.7097, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(361.8510, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(333.5999, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(396.8905, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(361.1466, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(360.0071, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(437.1695, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(349.9996, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(374.4934, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(384.3607, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(409.1148, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(421.7963, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(366.6469, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(404.5024, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(341.5081, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(344.8619, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(400.0934, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(380.7448, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(369.1128, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(345.6848, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(389.2040, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(353.6076, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(401.8952, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(406.7979, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(346.9375, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(390.5294, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(370.5898, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(304.5530, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(388.8476, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(355.8603, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(399.5678, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(376.1539, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(313.4066, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(382.3505, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(361.2434, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(367.5356, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(374.0271, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(373.1290, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(343.6791, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(368.3743, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(381.4538, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(395.2752, grad_fn=<AddBackward0>)\n",
      "Epoch 1 - Batch Loss: tensor(109.8421, grad_fn=<AddBackward0>)\n",
      "Epoch 1 finished. Loss: 52632.73828125\n",
      "Epoch 2 - Batch Loss: tensor(297.6418, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(320.7909, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(375.2864, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(328.0574, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(386.8483, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(367.9279, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(406.4214, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(366.4064, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(328.0019, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(371.1972, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(337.8352, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(341.3106, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(328.6833, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(334.0755, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(365.9197, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(340.1343, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(320.7549, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(369.0299, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(384.4414, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(365.9370, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(343.8166, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(311.0048, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(348.6524, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(361.1773, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(351.9955, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(366.5627, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(380.4087, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(332.4655, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(356.8324, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(351.4186, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(356.3330, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(390.8704, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(325.3513, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(336.7809, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(341.0822, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(386.4479, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(365.1288, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(393.2751, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(314.1802, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(339.3608, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(290.1662, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(350.3112, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(367.4037, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(359.8350, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(334.6099, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(346.7187, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(347.5726, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(294.0075, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(297.4497, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(348.9908, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(382.7415, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(337.7227, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(341.6767, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(330.4020, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(378.8324, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(306.7398, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(342.5340, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(325.5643, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(370.3983, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(358.8989, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(417.4573, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(390.7332, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(321.1646, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(376.0740, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(328.1003, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(390.1318, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(309.7983, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(368.3502, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(370.3044, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(338.4107, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(340.5101, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(332.4461, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(338.2984, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Batch Loss: tensor(336.4154, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(307.9500, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(351.8747, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(325.6076, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(359.4149, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(325.3698, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(364.0806, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(271.0013, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(335.1526, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(287.3306, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(354.4049, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(305.4586, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(321.3224, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(345.1018, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(282.8402, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(380.1490, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(350.8864, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(299.4182, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(367.5851, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(340.6634, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(355.6459, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(348.5626, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(332.3329, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(315.4824, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(339.5407, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(333.7310, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(347.6461, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(323.9629, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(381.8792, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(307.4110, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(304.9181, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(294.6294, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(324.9598, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(329.9554, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(313.2644, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(348.0446, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(347.9727, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(318.7332, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(359.2748, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(304.7627, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(361.2513, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(332.6462, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(390.4360, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(314.9752, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(379.8634, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(296.6487, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(359.0159, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(348.8579, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(290.2385, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(332.3846, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(316.8358, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(303.6102, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(305.7106, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(328.7131, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(385.7864, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(310.4422, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(279.1303, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(341.9121, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(309.9992, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(317.7817, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(334.0611, grad_fn=<AddBackward0>)\n",
      "Epoch 2 - Batch Loss: tensor(65.3007, grad_fn=<AddBackward0>)\n",
      "Epoch 2 finished. Loss: 45774.54052734375\n",
      "Epoch 3 - Batch Loss: tensor(265.0142, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(407.0090, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(316.7154, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(293.4976, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(374.0503, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(270.2744, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(337.9995, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(329.1557, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(346.7238, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(314.4319, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(343.4730, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(278.6447, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(335.2811, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(265.6009, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(365.8663, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(289.0048, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(335.4645, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(316.2090, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(319.5819, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(362.0686, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(319.5376, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(339.6941, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(294.7133, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(339.6422, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(320.4815, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(329.9575, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(323.5046, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(366.8545, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(296.0033, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(337.0511, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(343.3761, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(331.3531, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(360.1365, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(350.9036, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(269.4021, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(345.1961, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(349.3254, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(333.1820, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(329.1814, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(349.1777, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(356.1614, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(345.8061, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(309.1922, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(340.4151, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(310.5052, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(364.2159, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(315.3113, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(326.1664, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(370.9886, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(324.6884, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(381.6623, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(283.6187, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(362.6702, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(286.9518, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(345.3695, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(360.1620, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(323.6804, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(257.4152, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(320.1041, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(286.2926, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(315.5493, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(317.3853, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(327.7270, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(308.8217, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(364.7997, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(310.5412, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(353.0630, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(235.1647, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Batch Loss: tensor(292.4933, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(262.9567, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(309.8568, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(323.4774, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(339.7137, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(311.6755, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(318.4315, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(266.7117, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(355.2930, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(337.1050, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(299.9900, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(295.0881, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(368.1911, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(318.9061, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(278.9019, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(318.7212, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(345.6472, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(277.6326, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(328.9398, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(369.3394, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(310.6880, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(282.9883, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(343.2118, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(282.4105, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(357.4622, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(330.0396, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(278.4971, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(336.5186, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(320.4318, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(333.1017, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(311.9674, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(339.8440, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(275.9810, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(292.9608, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(313.4117, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(295.9617, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(351.8273, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(319.4658, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(317.2758, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(326.1360, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(281.7044, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(327.2396, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(316.0919, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(317.4511, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(336.4205, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(321.9711, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(294.0458, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(339.6662, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(283.0405, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(345.3438, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(340.7748, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(321.9118, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(282.4709, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(323.5353, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(330.6353, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(315.1084, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(324.2546, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(320.6768, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(315.5742, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(250.5051, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(295.5492, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(287.6383, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(314.2614, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(314.3148, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(294.7162, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(331.9532, grad_fn=<AddBackward0>)\n",
      "Epoch 3 - Batch Loss: tensor(78.4312, grad_fn=<AddBackward0>)\n",
      "Epoch 3 finished. Loss: 43043.708641052246\n",
      "Epoch 4 - Batch Loss: tensor(246.0767, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(297.3055, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(327.9469, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(294.9910, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(290.6788, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(323.0851, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(359.4240, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(330.0515, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(298.1966, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(330.8101, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(286.2525, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(316.3023, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(311.8165, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(311.9661, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(265.6988, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(341.5428, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(316.1593, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(366.9346, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(294.9686, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(310.6731, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(309.8065, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(344.9164, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(272.5078, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(345.1137, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(316.4027, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(317.8155, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(317.5001, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(401.1327, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(293.6346, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(278.2930, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(314.0212, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(285.3536, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(356.3557, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(330.9894, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(322.7120, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(284.6302, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(339.7991, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(290.0328, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(341.0513, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(289.3749, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(335.3537, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(266.3559, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(290.7350, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(310.1758, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(305.6598, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(293.4889, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(325.9828, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(329.8130, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(286.3358, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(278.9073, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(326.6245, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(336.6147, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(278.0993, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(316.4506, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(281.2083, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(343.8389, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(286.6759, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(316.3439, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(315.1404, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(314.6655, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(259.0835, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(330.3273, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(299.3101, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Batch Loss: tensor(273.2926, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(314.4807, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(326.3551, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(299.5879, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(298.8407, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(287.1249, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(269.4343, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(296.0606, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(322.7487, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(320.2121, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(282.1381, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(302.6798, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(300.3644, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(296.9041, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(293.6923, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(319.5908, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(320.2334, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(303.8811, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(337.1508, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(328.0482, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(296.2725, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(313.8266, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(345.9002, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(268.5698, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(278.3968, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(245.9876, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(322.4643, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(321.0752, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(324.9090, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(298.7710, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(270.0849, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(366.6230, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(307.6425, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(338.3611, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(345.1514, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(296.6831, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(270.3541, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(280.9899, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(268.5743, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(330.4518, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(272.9817, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(300.9160, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(360.8748, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(310.4126, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(318.2072, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(332.4035, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(326.4237, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(287.4957, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(333.4990, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(288.8025, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(286.0992, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(278.9893, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(332.8065, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(313.4283, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(324.2010, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(327.0092, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(301.4921, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(313.0206, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(332.7928, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(294.6746, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(340.3540, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(357.8088, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(246.9394, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(345.9868, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(248.8374, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(339.2840, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(297.2134, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(315.3415, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(277.7404, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(287.8339, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(274.5551, grad_fn=<AddBackward0>)\n",
      "Epoch 4 - Batch Loss: tensor(81.6208, grad_fn=<AddBackward0>)\n",
      "Epoch 4 finished. Loss: 41433.470458984375\n",
      "Epoch 5 - Batch Loss: tensor(283.4072, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(289.6206, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(267.7004, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(382.5684, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(225.1586, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(317.3542, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(315.5173, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(277.8492, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(309.4816, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(295.4236, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(275.9523, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(291.3388, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(299.2416, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(316.0340, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(327.8579, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(313.8437, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(279.0653, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(289.1390, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(315.8170, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(308.3232, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(297.9555, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(293.9556, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(255.2623, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(345.1749, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(252.3830, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(312.0096, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(288.2294, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(286.3280, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(286.4279, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(267.9781, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(326.0014, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(304.1311, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(325.3102, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(328.5717, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(324.8983, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(285.9420, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(300.8130, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(344.6161, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(287.5209, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(265.4329, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(291.7154, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(311.1718, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(282.2397, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(353.2723, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(283.9795, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(380.4431, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(290.6821, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(305.5515, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(269.9251, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(348.5964, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(265.8089, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(309.9225, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(272.2430, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(363.8095, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(298.2349, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(315.2119, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(248.0431, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(294.3150, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Batch Loss: tensor(275.8197, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(265.3690, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(324.5884, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(264.5491, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(304.4467, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(313.2188, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(367.8850, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(304.5409, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(322.9845, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(298.4960, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(327.7582, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(300.5813, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(287.0525, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(285.5741, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(285.6776, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(313.9114, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(269.9278, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(288.8645, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(332.7110, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(291.1176, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(321.4819, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(298.4849, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(275.6582, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(293.7089, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(279.7509, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(295.8974, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(308.3443, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(320.9574, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(297.0824, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(329.6587, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(280.7722, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(327.1190, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(281.5305, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(277.0341, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(306.5809, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(292.0414, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(292.7544, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(314.7840, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(313.1258, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(255.0166, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(268.8978, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(310.9210, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(313.1280, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(323.3129, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(279.2034, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(325.2983, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(243.8989, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(298.1256, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(308.2778, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(298.5580, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(277.9230, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(352.4340, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(270.8048, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(322.3879, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(270.6938, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(264.4711, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(304.0200, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(313.0367, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(284.9964, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(327.8033, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(261.6421, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(349.0067, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(306.7623, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(327.6794, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(291.6465, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(299.0557, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(308.2851, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(308.4286, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(265.3722, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(344.1657, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(275.2776, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(284.0793, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(299.1124, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(282.3832, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(324.6027, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(325.4387, grad_fn=<AddBackward0>)\n",
      "Epoch 5 - Batch Loss: tensor(73.8811, grad_fn=<AddBackward0>)\n",
      "Epoch 5 finished. Loss: 40316.671226501465\n",
      "Epoch 6 - Batch Loss: tensor(230.5401, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(331.5677, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(285.8410, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(301.9040, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(228.6581, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(321.5937, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(284.2719, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(356.5244, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(275.8941, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(293.1416, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(307.2215, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(291.3152, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(293.2004, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(318.8885, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(278.6631, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(275.7506, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(278.3167, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(346.9228, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(286.5019, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(308.4146, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(299.7635, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(339.5629, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(282.3749, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(262.3137, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(315.9236, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(271.9606, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(284.4565, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(342.3380, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(333.5776, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(287.0307, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(266.5052, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(337.8473, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(295.6584, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(346.9177, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(285.8218, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(312.8406, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(246.1315, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(310.8960, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(247.7507, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(300.6029, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(268.2985, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(349.4949, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(302.1380, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(323.4858, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(221.6692, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(328.1177, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(205.1570, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(257.8600, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(269.3443, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(335.8631, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(325.6330, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(319.9016, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(290.4965, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Batch Loss: tensor(320.1781, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(295.6985, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(276.6879, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(259.2893, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(322.4726, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(261.8762, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(291.0500, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(262.1798, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(335.5339, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(302.7899, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(266.9153, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(293.7153, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(336.5323, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(312.5788, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(341.5430, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(276.4040, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(331.5276, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(261.7182, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(298.9372, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(271.9508, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(288.1713, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(277.6049, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(321.2277, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(269.0271, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(346.8207, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(295.3007, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(300.8425, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(305.6970, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(303.8446, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(280.7624, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(324.7574, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(304.6352, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(276.7385, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(286.7567, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(336.4907, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(320.9310, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(302.7594, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(286.1426, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(284.6243, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(315.6334, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(279.3099, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(281.5958, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(308.3665, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(236.1219, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(299.0188, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(251.6718, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(268.7625, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(299.8000, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(266.1158, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(320.7121, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(237.3796, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(300.0823, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(276.8824, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(302.0968, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(269.5602, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(319.3524, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(328.1859, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(319.0859, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(294.9615, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(285.8744, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(282.7054, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(256.8076, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(313.9037, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(271.5827, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(303.6121, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(291.9659, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(294.4614, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(301.8629, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(292.7685, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(273.1854, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(326.9595, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(289.9472, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(314.4690, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(286.7774, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(288.1199, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(360.9607, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(258.8868, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(280.1561, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(318.1418, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(286.7131, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(271.5519, grad_fn=<AddBackward0>)\n",
      "Epoch 6 - Batch Loss: tensor(84.6502, grad_fn=<AddBackward0>)\n",
      "Epoch 6 finished. Loss: 39606.33824920654\n",
      "Epoch 7 - Batch Loss: tensor(234.4542, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(324.5811, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(320.7169, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(270.0099, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(302.7287, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(311.2571, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(251.2436, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(289.4963, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(342.1106, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(266.9019, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(273.7349, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(303.6067, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(299.1863, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(349.5368, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(268.2447, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(314.2915, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(349.5830, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(306.5263, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(278.8897, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(364.3935, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(279.2205, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(290.2414, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(266.6648, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(335.6604, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(285.8441, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(316.7937, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(262.8720, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(309.3747, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(291.3795, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(283.1871, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(222.4227, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(350.3149, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(268.0435, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(319.3899, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(240.1292, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(343.5542, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(280.3456, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(261.0148, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(305.0552, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(273.7085, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(278.2313, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(344.0720, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(249.5894, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(294.6208, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(258.4542, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(339.3727, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(255.4984, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(349.4992, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Batch Loss: tensor(343.4206, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(256.0244, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(295.6460, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(292.9371, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(255.1159, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(335.8193, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(296.2448, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(302.3138, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(306.9272, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(285.4006, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(308.4241, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(295.9642, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(296.9765, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(308.5751, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(276.1827, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(285.6741, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(272.9978, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(300.4691, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(268.6991, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(318.8672, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(298.8307, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(289.0160, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(259.0582, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(294.5652, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(291.8551, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(284.9868, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(229.5126, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(356.8969, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(269.2232, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(281.8910, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(293.1082, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(264.4644, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(270.4400, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(285.9449, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(274.1151, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(304.5706, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(280.1893, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(313.9504, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(285.7029, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(251.3127, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(277.0390, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(282.1679, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(298.4885, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(281.9799, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(307.3209, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(261.0927, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(319.6044, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(282.6849, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(274.1256, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(287.3159, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(298.1212, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(262.0187, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(252.0651, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(304.1429, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(307.4538, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(284.9709, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(284.6629, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(311.9507, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(285.7799, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(306.2975, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(280.6075, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(293.7027, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(277.8525, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(296.0459, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(260.1302, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(308.4120, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(277.0054, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(248.8740, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(327.4420, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(310.9099, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(294.8750, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(338.2258, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(298.0832, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(360.5115, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(227.1721, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(310.2223, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(286.8083, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(321.0448, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(247.7793, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(322.3140, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(300.6468, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(266.8115, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(313.7269, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(321.3505, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(269.0520, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(280.9832, grad_fn=<AddBackward0>)\n",
      "Epoch 7 - Batch Loss: tensor(67.4381, grad_fn=<AddBackward0>)\n",
      "Epoch 7 finished. Loss: 39235.6727142334\n",
      "Epoch 8 - Batch Loss: tensor(234.7477, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(351.2297, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(273.7613, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(278.2467, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(329.9539, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(304.7419, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(244.7136, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(307.8947, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(256.3472, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(320.8152, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(311.1185, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(334.4966, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(283.3592, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(280.0914, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(265.9720, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(309.1027, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(289.4181, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(294.3254, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(264.1541, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(300.2229, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(270.0233, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(297.8802, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(258.1439, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(326.3518, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(266.1563, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(287.2096, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(302.9768, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(258.7189, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(269.9625, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(295.7097, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(328.5007, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(279.8845, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(272.1682, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(284.7082, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(284.0335, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(317.8893, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(269.8505, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(290.5087, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(234.9021, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(331.9660, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(276.2743, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(324.6385, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(236.8055, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Batch Loss: tensor(317.2844, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(273.1647, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(307.0386, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(268.4367, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(256.0028, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(294.3674, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(316.2577, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(276.6487, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(351.4377, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(245.3644, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(317.2142, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(253.9274, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(293.8730, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(289.8570, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(316.7290, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(290.7073, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(303.9128, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(199.1259, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(330.0421, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(198.6834, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(313.0844, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(251.0633, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(346.8995, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(224.8710, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(308.8902, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(270.2902, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(332.9755, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(240.4485, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(291.6236, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(301.7570, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(313.6711, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(252.2487, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(349.8083, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(256.2705, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(324.0273, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(290.0676, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(336.1222, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(316.8292, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(268.3794, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(349.1800, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(270.0139, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(349.0172, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(258.6830, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(306.0863, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(263.8680, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(314.4605, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(261.4117, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(261.1733, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(306.1222, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(329.5628, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(308.9948, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(242.7789, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(282.4167, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(290.3857, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(287.9395, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(263.1148, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(288.5859, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(305.1411, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(313.7377, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(269.3778, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(322.3941, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(305.2687, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(290.2671, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(261.2853, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(269.7972, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(263.2379, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(313.8622, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(299.0326, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(318.6742, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(251.5820, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(304.8077, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(253.4177, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(330.9268, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(260.5249, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(292.8441, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(291.1925, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(309.9305, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(278.1932, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(246.3035, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(325.3474, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(293.9625, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(279.9321, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(293.7943, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(289.7132, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(294.1626, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(244.0494, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(302.6120, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(247.4708, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(291.7765, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(311.7158, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(280.4603, grad_fn=<AddBackward0>)\n",
      "Epoch 8 - Batch Loss: tensor(79.2602, grad_fn=<AddBackward0>)\n",
      "Epoch 8 finished. Loss: 38809.22978973389\n",
      "Epoch 9 - Batch Loss: tensor(229.0056, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(319.5264, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(211.2627, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(292.4875, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(289.2903, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(271.4475, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(228.6805, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(311.8951, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(331.9464, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(276.4387, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(292.1593, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(273.9890, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(259.6178, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(255.5098, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(299.5935, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(322.0753, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(260.6210, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(325.7637, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(274.7123, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(280.0814, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(314.6060, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(309.3179, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(299.3831, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(309.5061, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(261.0468, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(267.8442, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(286.6581, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(306.1509, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(290.5624, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(321.9156, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(241.7104, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(304.3808, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(326.1324, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(305.0934, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(268.5686, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(292.3863, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(267.8113, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(331.9950, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Batch Loss: tensor(256.5023, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(262.7022, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(318.0620, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(285.1078, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(260.4895, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(320.1684, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(240.4922, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(284.2398, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(275.8812, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(274.2721, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(309.2077, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(315.1269, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(271.4916, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(337.6102, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(280.0358, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(301.0800, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(239.8673, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(294.6335, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(293.8526, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(331.7309, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(262.4255, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(371.9896, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(247.9325, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(295.8574, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(289.1833, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(292.8506, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(271.3992, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(306.1702, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(225.5185, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(327.9921, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(263.4391, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(348.2213, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(283.5387, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(323.3969, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(294.4995, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(327.1115, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(248.1333, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(239.6500, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(298.3622, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(258.5022, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(319.6533, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(286.2371, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(308.4549, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(267.0432, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(283.6888, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(278.4723, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(308.6848, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(275.2250, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(259.7096, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(313.2886, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(245.3864, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(305.4550, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(232.5045, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(310.1223, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(231.6355, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(347.2013, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(254.2963, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(290.7616, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(263.8158, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(320.0277, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(274.7949, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(306.4536, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(283.7061, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(286.1412, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(261.6252, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(286.4256, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(306.1815, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(259.7721, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(310.8788, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(274.2990, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(291.1994, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(289.3844, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(265.8694, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(289.9541, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(246.9107, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(311.6640, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(279.4174, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(296.4352, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(249.9153, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(267.9054, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(326.5091, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(298.2845, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(294.2702, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(310.1852, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(307.7021, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(315.2716, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(273.6275, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(321.7588, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(241.7477, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(339.6609, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(231.7781, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(335.8237, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(288.4688, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(306.9666, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(239.4426, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(265.3777, grad_fn=<AddBackward0>)\n",
      "Epoch 9 - Batch Loss: tensor(88.0005, grad_fn=<AddBackward0>)\n",
      "Epoch 9 finished. Loss: 38565.373390197754\n",
      "After Trainning(First Text): tensor([-0.2257, -0.2069, -0.1890,  0.0949, -0.0418,  0.1746, -0.0699,  0.1981,\n",
      "         0.0105, -0.0788, -0.2114, -0.1839,  0.1660, -0.2282, -0.3175, -0.2853,\n",
      "        -0.2831, -0.0184, -0.0931, -0.2646,  0.2686, -0.4916, -0.2039,  0.2962,\n",
      "         0.1197, -0.4686,  0.0143, -0.0496,  0.2560, -0.3231, -0.3294, -0.0338,\n",
      "         0.2137,  0.0952, -0.0669, -0.0238,  0.1799, -0.2170,  0.2854,  0.0182,\n",
      "        -0.0955,  0.1069,  0.2421, -0.0130, -0.3776, -0.0621,  0.0351,  0.2888,\n",
      "        -0.0451,  0.3833,  0.1113,  0.1978, -0.0224,  0.0429, -0.1056, -0.0648,\n",
      "        -0.3080,  0.0636, -0.1800,  0.3061,  0.0986, -0.1727, -0.5064,  0.4789,\n",
      "         0.0640, -0.0519, -0.0743, -0.0853, -0.1325, -0.0241, -0.5257,  0.1210,\n",
      "        -0.3653,  0.0516,  0.0405, -0.0916, -0.4372,  0.2872, -0.3748, -0.2332,\n",
      "         0.2002,  0.1791,  0.1591, -0.1416,  0.1241,  0.1186,  0.4286,  0.2661,\n",
      "         0.2387, -0.0742,  0.0618,  0.1640, -0.1391, -0.0396,  0.4437,  0.3330,\n",
      "        -0.0458,  0.2237,  0.0898, -0.2275, -0.1513, -0.0898,  0.0564, -0.4281,\n",
      "        -0.2539,  0.0443, -0.0441,  0.0616, -0.0617, -0.1591,  0.2231, -0.1361,\n",
      "        -0.3596,  0.0024, -0.0753,  0.0727,  0.3240,  0.1493, -0.2064, -0.1179,\n",
      "        -0.0057, -0.1040,  0.0430,  0.0998, -0.0895, -0.0206, -0.2067,  0.1008,\n",
      "         0.3698,  0.1919,  0.0591, -0.3218,  0.1226, -0.0299, -0.2218, -0.0107,\n",
      "        -0.1809,  0.1938,  0.0423, -0.2539,  0.1758, -0.0150, -0.4190, -0.2069,\n",
      "        -0.0493, -0.1538,  0.3590,  0.1187,  0.3704,  0.3061, -0.1622,  0.0133,\n",
      "        -0.0770,  0.2360, -0.3069, -0.2123, -0.0166, -0.0805,  0.0150, -0.1738,\n",
      "        -0.0311,  0.0466,  0.1240, -0.1287, -0.0783,  0.4820,  0.2004,  0.2226,\n",
      "        -0.2074,  0.1306, -0.0055, -0.4788, -0.4106,  0.1492,  0.3890,  0.2051,\n",
      "        -0.1289,  0.5208,  0.3118,  0.2457,  0.3477, -0.3044,  0.0920, -0.2835,\n",
      "        -0.3403,  0.2487, -0.2225,  0.0987,  0.2076,  0.0204,  0.2008,  0.5172,\n",
      "        -0.0107, -0.2285,  0.1874,  0.0080,  0.2701,  0.1283, -0.3072,  0.0185,\n",
      "         0.1595,  0.0663, -0.3109,  0.0208, -0.1709,  0.4597, -0.3434, -0.0614,\n",
      "         0.3097,  0.0897,  0.4345, -0.1596,  0.4554, -0.4194, -0.0066, -0.7030,\n",
      "        -0.0955, -0.0745, -0.5044, -0.0174, -0.0564, -0.1727, -0.2801, -0.3355,\n",
      "         0.0036, -0.2025, -0.1877, -0.0217, -0.3907, -0.0029, -0.1708,  0.0894,\n",
      "         0.1894, -0.1850,  0.1279, -0.4512, -0.5965,  0.0218, -0.0115,  0.0147,\n",
      "        -0.1168, -0.2287, -0.1850, -0.0711, -0.1243,  0.0627,  0.2224,  0.2328,\n",
      "         0.0548,  0.0459,  0.0898,  0.1564,  0.0908, -0.0047,  0.2934,  0.0408,\n",
      "        -0.0184,  0.0561, -0.0816, -0.0372,  0.2261,  0.1509, -0.2274,  0.1705,\n",
      "        -0.1173,  0.2538, -0.0598, -0.0187, -0.0285,  0.0657, -0.4180, -0.1623,\n",
      "        -0.4527,  0.0885, -0.0825,  0.5355,  0.1846, -0.2086, -0.0568, -0.0147,\n",
      "        -0.2790,  0.4317,  0.0534,  0.4428, -0.0306,  0.0883, -0.2951, -0.0172,\n",
      "         0.1348, -0.5315,  0.0777,  0.0127,  0.2530,  0.0233,  0.1118, -0.3227,\n",
      "        -0.1305,  0.4068,  0.1911,  0.2424], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2159b6b7730>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LossList = []\n",
    "parameter = {**label_tensor,**word_tensor,**relation_tensor}.values()\n",
    "optimizer = optim.SGD(parameter,lr=0.005)\n",
    "batch_size = 512\n",
    "print('Before Trainning(First Text):',word_tensor['深圳'])\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    shuffle(allfacts)\n",
    "    list2 = []\n",
    "    for i in range(0, len(allfacts), batch_size):\n",
    "        list2.append(allfacts[i:i+batch_size])\n",
    "    for sub_batch in list2:\n",
    "        optimizer.zero_grad()\n",
    "        liu = Loss(sub_batch)\n",
    "        # print(type(liu))\n",
    "        print('Epoch',epoch,'- Batch Loss:',liu)\n",
    "        epoch_loss += float(liu)\n",
    "        liu.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    print('Epoch',epoch,'finished. Loss:',epoch_loss)\n",
    "    LossList.append(epoch_loss)\n",
    "    \n",
    "print('After Trainning(First Text):',word_tensor['深圳'])\n",
    "plt.grid(True)\n",
    "plt.plot(range(len(LossList)),LossList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {**label_tensor,**word_tensor,**relation_tensor}\n",
    "f = open('model/kge1.kge','wb')\n",
    "pickle.dump(final,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2257, -0.2069, -0.1890,  0.0949, -0.0418,  0.1746, -0.0699,  0.1981,\n",
       "         0.0105, -0.0788, -0.2114, -0.1839,  0.1660, -0.2282, -0.3175, -0.2853,\n",
       "        -0.2831, -0.0184, -0.0931, -0.2646,  0.2686, -0.4916, -0.2039,  0.2962,\n",
       "         0.1197, -0.4686,  0.0143, -0.0496,  0.2560, -0.3231, -0.3294, -0.0338,\n",
       "         0.2137,  0.0952, -0.0669, -0.0238,  0.1799, -0.2170,  0.2854,  0.0182,\n",
       "        -0.0955,  0.1069,  0.2421, -0.0130, -0.3776, -0.0621,  0.0351,  0.2888,\n",
       "        -0.0451,  0.3833,  0.1113,  0.1978, -0.0224,  0.0429, -0.1056, -0.0648,\n",
       "        -0.3080,  0.0636, -0.1800,  0.3061,  0.0986, -0.1727, -0.5064,  0.4789,\n",
       "         0.0640, -0.0519, -0.0743, -0.0853, -0.1325, -0.0241, -0.5257,  0.1210,\n",
       "        -0.3653,  0.0516,  0.0405, -0.0916, -0.4372,  0.2872, -0.3748, -0.2332,\n",
       "         0.2002,  0.1791,  0.1591, -0.1416,  0.1241,  0.1186,  0.4286,  0.2661,\n",
       "         0.2387, -0.0742,  0.0618,  0.1640, -0.1391, -0.0396,  0.4437,  0.3330,\n",
       "        -0.0458,  0.2237,  0.0898, -0.2275, -0.1513, -0.0898,  0.0564, -0.4281,\n",
       "        -0.2539,  0.0443, -0.0441,  0.0616, -0.0617, -0.1591,  0.2231, -0.1361,\n",
       "        -0.3596,  0.0024, -0.0753,  0.0727,  0.3240,  0.1493, -0.2064, -0.1179,\n",
       "        -0.0057, -0.1040,  0.0430,  0.0998, -0.0895, -0.0206, -0.2067,  0.1008,\n",
       "         0.3698,  0.1919,  0.0591, -0.3218,  0.1226, -0.0299, -0.2218, -0.0107,\n",
       "        -0.1809,  0.1938,  0.0423, -0.2539,  0.1758, -0.0150, -0.4190, -0.2069,\n",
       "        -0.0493, -0.1538,  0.3590,  0.1187,  0.3704,  0.3061, -0.1622,  0.0133,\n",
       "        -0.0770,  0.2360, -0.3069, -0.2123, -0.0166, -0.0805,  0.0150, -0.1738,\n",
       "        -0.0311,  0.0466,  0.1240, -0.1287, -0.0783,  0.4820,  0.2004,  0.2226,\n",
       "        -0.2074,  0.1306, -0.0055, -0.4788, -0.4106,  0.1492,  0.3890,  0.2051,\n",
       "        -0.1289,  0.5208,  0.3118,  0.2457,  0.3477, -0.3044,  0.0920, -0.2835,\n",
       "        -0.3403,  0.2487, -0.2225,  0.0987,  0.2076,  0.0204,  0.2008,  0.5172,\n",
       "        -0.0107, -0.2285,  0.1874,  0.0080,  0.2701,  0.1283, -0.3072,  0.0185,\n",
       "         0.1595,  0.0663, -0.3109,  0.0208, -0.1709,  0.4597, -0.3434, -0.0614,\n",
       "         0.3097,  0.0897,  0.4345, -0.1596,  0.4554, -0.4194, -0.0066, -0.7030,\n",
       "        -0.0955, -0.0745, -0.5044, -0.0174, -0.0564, -0.1727, -0.2801, -0.3355,\n",
       "         0.0036, -0.2025, -0.1877, -0.0217, -0.3907, -0.0029, -0.1708,  0.0894,\n",
       "         0.1894, -0.1850,  0.1279, -0.4512, -0.5965,  0.0218, -0.0115,  0.0147,\n",
       "        -0.1168, -0.2287, -0.1850, -0.0711, -0.1243,  0.0627,  0.2224,  0.2328,\n",
       "         0.0548,  0.0459,  0.0898,  0.1564,  0.0908, -0.0047,  0.2934,  0.0408,\n",
       "        -0.0184,  0.0561, -0.0816, -0.0372,  0.2261,  0.1509, -0.2274,  0.1705,\n",
       "        -0.1173,  0.2538, -0.0598, -0.0187, -0.0285,  0.0657, -0.4180, -0.1623,\n",
       "        -0.4527,  0.0885, -0.0825,  0.5355,  0.1846, -0.2086, -0.0568, -0.0147,\n",
       "        -0.2790,  0.4317,  0.0534,  0.4428, -0.0306,  0.0883, -0.2951, -0.0172,\n",
       "         0.1348, -0.5315,  0.0777,  0.0127,  0.2530,  0.0233,  0.1118, -0.3227,\n",
       "        -0.1305,  0.4068,  0.1911,  0.2424], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('model/kge1.kge','rb')\n",
    "liu = pickle.load(f)\n",
    "f.close()\n",
    "liu['深圳']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'q':1,'w':2,'e':3}\n",
    "b = {'w':1.5,'s':2.5}\n",
    "c = {**b,**a}\n",
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
